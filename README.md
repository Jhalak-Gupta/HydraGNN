#HydraGNN
Graph Neural Networks (GNNs) are increasingly deployed in critical domains, yet their susceptibility to camouflaged poisoning attacks remains a pressing concern. These attacks are difficult to counter because adversarial nodes and edges imitate the statistical patterns of genuine graph data, rendering conventional detection methods ineffective. We introduce HydraGNN, a scalable defense that unifies four orthogonal strategies into a single end-to-end pipeline: edge pruning to excise overtly malicious links, Bayesian uncertainty estimation via Monte Carlo dropout to quantify prediction robustness, anomaly detection, and randomized smoothing to enhance resilience against camouflaged perturbations. We evaluate HydraGNN on benchmark citation datasets, including Cora, CiteSeer and PubMed under state-of-the-art camouflaged attackers. HydraGNN consistently reduces Attack Success Rate (ASR) by up to 56%, while incurring less than 1% clean accuracy drop across canonical base models (GCN and GraphSAGE), and next-generation architectures (GPR and GAT). These results underscore the importance of coordinated, multi-strategy defenses for advancing adversarial robustness in contemporary graph learning systems. 

